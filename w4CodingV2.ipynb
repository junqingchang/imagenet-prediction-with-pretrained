{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.model_zoo as model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested on Python version 3.6.8 and PyTorch version 1.0.0\n",
    "# Machine used: Macbook with no CUDA\n",
    "\n",
    "SYNSET_WORDS = 'imagenet_first2500/synset_words.txt'\n",
    "# ASSUMES IMAGES ARE JPEG FORMAT\n",
    "IMAGE_DIRECTORY = 'imagenet_first2500/imagespart/'\n",
    "XML_DIRECTORY = 'imagenet_first2500/DONOTEXPAND/'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class ImageNetData(Dataset):\n",
    "    def __init__(self, synset, image_directory, xml_directory, transform = None):\n",
    "        f = open(synset,\"r\")\n",
    "        self.label = {}\n",
    "        ctr = 0\n",
    "        self.index_to_class = {}\n",
    "        self.name_to_index = {}\n",
    "        for line in f:\n",
    "            self.label[line[0:9]] = line[10:-1]\n",
    "            self.index_to_class[ctr] = line[10:-1]\n",
    "            self.name_to_index[line[0:9]] = ctr\n",
    "            ctr += 1\n",
    "        self.image_list = os.listdir(image_directory)\n",
    "        self.image_path = image_directory\n",
    "        self.xml_path = xml_directory\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        filename, file_extension = os.path.splitext(self.image_list[idx])\n",
    "        item = self.from_xml(filename + '.xml')\n",
    "        image = io.imread(self.image_path+ filename + '.JPEG')\n",
    "        target = self.name_to_index[item]\n",
    "        if(len(image.shape) == 2):\n",
    "            image = np.stack((image,)*3, axis=-1)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return (image, target)\n",
    "        \n",
    "    def from_xml(self, reference):\n",
    "        root = ET.parse(self.xml_path + reference).getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            for name in obj.findall('name'):\n",
    "                output_name = name.text \n",
    "        return output_name\n",
    "    \n",
    "    def nm_to_idx(self, nm):\n",
    "        return self.name_to_index[nm]\n",
    "    \n",
    "    def idx_to_class(self, idx):\n",
    "        return self.index_to_class[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, log_freq = 100):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    ctr = 0\n",
    "    is_multi = False\n",
    "    num_crop = 1\n",
    "    total_num_data = len(test_loader.dataset)\n",
    "#     expanded_set = 1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if len(data.shape) > 4:\n",
    "                num_crop = data.shape[1]\n",
    "                is_multi = True\n",
    "#                 target = target.view(-1, 1).repeat(1, data.shape[1]).view(1,-1)\n",
    "#                 expanded_set = data.shape[1]\n",
    "                data = data.view([-1,data.shape[-3],data.shape[-2],data.shape[-1]])  \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            if is_multi:\n",
    "                output = output.view(test_loader.batch_size, num_crop, -1)\n",
    "                output = output.mean(1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            ctr += 1\n",
    "            if ctr % log_freq == 0:\n",
    "                print(\"Currently on Image {}\".format(ctr*test_loader.num_workers))\n",
    "                 \n",
    "              # Break at 500 images  \n",
    "#             if ctr*test_loader.batch_size >= 500:\n",
    "#                 break\n",
    "                \n",
    "#     total_num_data *= expanded_set\n",
    "    accuracy = 100. * correct / total_num_data\n",
    "    \n",
    "\n",
    "    print('\\nPerformance: Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        correct, total_num_data, accuracy))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dllesson/lib/python3.6/site-packages/torchvision/models/squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "/anaconda3/envs/dllesson/lib/python3.6/site-packages/torchvision/models/squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    }
   ],
   "source": [
    "squeezenet = models.squeezenet1_1(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center Crop with no Normalize\n",
      "Currently on Image 400\n",
      "Currently on Image 800\n",
      "Currently on Image 1200\n",
      "Currently on Image 1600\n",
      "Currently on Image 2000\n",
      "Currently on Image 2400\n",
      "\n",
      "Performance: Accuracy: 340/2500 (13.60%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centercrop_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "transformed_dataset = ImageNetData(SYNSET_WORDS,IMAGE_DIRECTORY,XML_DIRECTORY, centercrop_transform)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "print(\"Center Crop with no Normalize\")\n",
    "test(squeezenet, device, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center Crop with Normalize\n",
      "Currently on Image 400\n",
      "Currently on Image 800\n",
      "Currently on Image 1200\n",
      "Currently on Image 1600\n",
      "Currently on Image 2000\n",
      "Currently on Image 2400\n",
      "\n",
      "Performance: Accuracy: 1442/2500 (57.68%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57.68"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centercropnorm_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "transformed_dataset = ImageNetData(SYNSET_WORDS,IMAGE_DIRECTORY,XML_DIRECTORY, centercropnorm_transform)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "print(\"Center Crop with Normalize\")\n",
    "test(squeezenet, device, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five Crop with Normalize\n",
      "Currently on Image 400\n",
      "Currently on Image 800\n",
      "Currently on Image 1200\n",
      "Currently on Image 1600\n",
      "Currently on Image 2000\n",
      "Currently on Image 2400\n",
      "\n",
      "Performance: Accuracy: 1536/2500 (61.44%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61.44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivecrop_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(280),\n",
    "        transforms.FiveCrop(224), \n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])(norm) for norm in norms]))\n",
    "    ])\n",
    "transformed_dataset = ImageNetData(SYNSET_WORDS,IMAGE_DIRECTORY,XML_DIRECTORY, fivecrop_transform)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "print(\"Five Crop with Normalize\")\n",
    "test(squeezenet, device, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze330(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(Squeeze330,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.newPool = nn.AdaptiveAvgPool2d((224,224))\n",
    "        self.reference = models.squeezenet1_1(pretrained = True)\n",
    "    def forward(self, x):\n",
    "        x = self.newPool(x)\n",
    "        x = self.reference.features(x)\n",
    "        x = self.reference.classifier(x)\n",
    "        return x.view(x.size(0), self.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze330 = Squeeze330()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size 330 with SqueezeNet\n",
      "Currently on Image 400\n",
      "Currently on Image 800\n",
      "Currently on Image 1200\n",
      "Currently on Image 1600\n",
      "Currently on Image 2000\n",
      "Currently on Image 2400\n",
      "\n",
      "Performance: Accuracy: 1509/2500 (60.36%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.36"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(380),\n",
    "        transforms.FiveCrop(330), \n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])(norm) for norm in norms]))\n",
    "    ])\n",
    "transformed_dataset = ImageNetData(SYNSET_WORDS,IMAGE_DIRECTORY,XML_DIRECTORY, task3_transform)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "print(\"Input Size 330 with SqueezeNet\")\n",
    "test(squeeze330, device, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception330(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
    "        super(Inception330, self).__init__()\n",
    "        self.newPool = nn.AdaptiveAvgPool2d((224,224))\n",
    "        self.reference = models.inception_v3(pretrained=True)\n",
    "    def forward(self, x):\n",
    "        features = self.newPool(x)\n",
    "        out = self.reference.forward(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception330 = Inception330()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size 330 with DenseNet\n",
      "Currently on Image 400\n",
      "Currently on Image 800\n",
      "Currently on Image 1200\n",
      "Currently on Image 1600\n",
      "Currently on Image 2000\n",
      "Currently on Image 2400\n",
      "\n",
      "Performance: Accuracy: 1863/2500 (74.52%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(380),\n",
    "        transforms.FiveCrop(330), \n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])(norm) for norm in norms]))\n",
    "    ])\n",
    "transformed_dataset = ImageNetData(SYNSET_WORDS,IMAGE_DIRECTORY,XML_DIRECTORY, task3_transform)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "print(\"Input Size 330 with Inception v3\")\n",
    "test(inception330, device, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
